{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLf0OZHpFyta",
        "outputId": "97c97e9b-9c27-459d-e187-07ca6f6c2db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 12 06:07:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    39W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR10nF2QYIs1",
        "outputId": "58458337-9eeb-43b7-a4c0-8884412dc7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2r-1rbNhWdtF"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import numpy as np\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers , Sequential\n",
        "from tensorflow.keras.layers import Conv2D ,MaxPooling2D ,Flatten ,Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "373-Tu9XYCJe"
      },
      "outputs": [],
      "source": [
        "S=7\n",
        "B=2\n",
        "C=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-_tFQzCnYGtA"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/yolo/fire_git/train'\n",
        "val_path = '/content/drive/MyDrive/yolo/fire_git/val'\n",
        "\n",
        "train_image_path_list= [x for x in glob(train_path+'/images' + '/*.jpg' )]\n",
        "train_image_path_list += [x for x in glob(train_path+'/images' + '/*.png' )]\n",
        "train_image_path_list=sorted(train_image_path_list)\n",
        "\n",
        "train_label_path_list=sorted([x for x in glob(train_path +'/labels' +'/*.txt')])\n",
        "\n",
        "val_image_path_list= [x for x in glob(val_path+'/images' + '/*.jpg' )]\n",
        "val_image_path_list += [x for x in glob(val_path+'/images' + '/*.png' )]\n",
        "val_image_path_list = sorted(val_image_path_list)\n",
        "\n",
        "val_label_path_list= sorted([x for x in glob(val_path +'/labels' +'/*.txt')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FV_WDisEZ03C"
      },
      "outputs": [],
      "source": [
        "# 라벨링 좌표를 이중 리스트로 출력. 입력 txt (xywh)\n",
        "def xywh(label_path_list):\n",
        "    train_label_list= []\n",
        "    for x in label_path_list:\n",
        "        f= open(x,'r')\n",
        "        strings = f.readlines()\n",
        "        train_label_list.append(strings)\n",
        "\n",
        "    return train_label_list\n",
        "    \n",
        "def labeling(label_list):\n",
        "    label_all=[]\n",
        "    x = 0\n",
        "    y = 0\n",
        "    x_grid = 0\n",
        "    y_grid = 0\n",
        "    count= 0\n",
        "    for i in label_list:\n",
        "        label = np.zeros((S,S,B*5+C),dtype = float)\n",
        "        \n",
        "        for j in range(len(i)):\n",
        "            coord=i[j].split()\n",
        "            x = float(coord[1])\n",
        "            y = float(coord[2])\n",
        "            x_grid = int(x*S) # 0~6\n",
        "            y_grid = int(y*S) # 0~6\n",
        "\n",
        "            cls= coord[0]\n",
        "            label[y_grid][x_grid][0] = 1\n",
        "            label[y_grid][x_grid][10+int(cls)] = 1\n",
        "\n",
        "            x_in_cell=x*S-x_grid\n",
        "            y_in_cell=y*S-y_grid\n",
        "            w=coord[3]\n",
        "            h=coord[4]\n",
        "            label[y_grid][x_grid][1:5]=[x_in_cell,y_in_cell,w,h]\n",
        "\n",
        "        label_all.append(label)\n",
        "    label_all = np.reshape(label_all,(-1,S,S,5*B+C)).astype(np.float32)\n",
        "    return label_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nKfnZDd9FH9v"
      },
      "outputs": [],
      "source": [
        "def dataset(train_image_path_list):\n",
        "    result = []\n",
        "\n",
        "    for image_path in train_image_path_list:\n",
        "        image = load_img(image_path,target_size = (224,224))\n",
        "        input_arr = img_to_array(image).astype(np.float32)/255.0\n",
        "        result.append(input_arr)\n",
        "    result =np.reshape(result,(-1,224,224,3))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fDklp_sQHL4G"
      },
      "outputs": [],
      "source": [
        "train_image_path_list_selected = []\n",
        "labels_list_train_selected= []\n",
        "val_image_path_list_selected= []\n",
        "labels_list_val_selected = []\n",
        "\n",
        "for image in train_image_path_list:\n",
        "    for label in train_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            train_image_path_list_selected.append(image)\n",
        "            labels_list_train_selected.append(label)\n",
        "            break\n",
        "for image in val_image_path_list:\n",
        "    for label in val_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            val_image_path_list_selected.append(image)\n",
        "            labels_list_val_selected.append(label)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7sdlfou0GDVp"
      },
      "outputs": [],
      "source": [
        "train_image_path_list_selected=train_image_path_list_selected\n",
        "labels_list_train_selected=labels_list_train_selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "70Tf1D5JaGeJ"
      },
      "outputs": [],
      "source": [
        "train = dataset(train_image_path_list_selected)\n",
        "val = dataset(val_image_path_list_selected)\n",
        "\n",
        "labels_train_selected=xywh(labels_list_train_selected)\n",
        "labels_val_selected = xywh(labels_list_val_selected)\n",
        "\n",
        "labels_train =labeling(labels_train_selected)\n",
        "labels_val=labeling(labels_val_selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvEmOrglFEzw",
        "outputId": "b389f284-86ab-4825-f514-1c1c58b12d7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3522, 3522, 150, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(train), len(labels_train),len(val) ,len(labels_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mul-MQBE0-Q"
      },
      "source": [
        "추가 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84j98C-8JZk4",
        "outputId": "e0754e65-c68b-4ce0-eebd-0fddebdc313e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3936, 3936, 180, 180)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def labeling_1(label_list):# 클래스 그냥 0으로 집어넣는 함수 ( 이 데이터는 클래스가 1로 되어있음)\n",
        "    label_all=[]\n",
        "    x = 0\n",
        "    y = 0\n",
        "    x_grid = 0\n",
        "    y_grid = 0\n",
        "    count= 0\n",
        "    for i in label_list:\n",
        "        label = np.zeros((S,S,B*5+C),dtype = float)\n",
        "        \n",
        "        for j in range(len(i)):\n",
        "            coord=i[j].split()\n",
        "            x = float(coord[1])\n",
        "            y = float(coord[2])\n",
        "            x_grid = int(x*S) # 0~6\n",
        "            y_grid = int(y*S) # 0~6\n",
        "\n",
        "            cls= coord[0]\n",
        "            label[y_grid][x_grid][0] = 1\n",
        "            label[y_grid][x_grid][10] = 1\n",
        "\n",
        "            x_in_cell=x*S-x_grid\n",
        "            y_in_cell=y*S-y_grid\n",
        "            w=coord[3]\n",
        "            h=coord[4]\n",
        "            label[y_grid][x_grid][1:5]=[x_in_cell,y_in_cell,w,h]\n",
        "\n",
        "        label_all.append(label)\n",
        "    label_all = np.reshape(label_all,(-1,S,S,5*B+C)).astype(np.float32)\n",
        "    return label_all\n",
        "train_path = '/content/drive/MyDrive/yolo/fire_pictures/train'\n",
        "val_path = '/content/drive/MyDrive/yolo/fire_pictures/val'\n",
        "\n",
        "train_image_path_list= [x for x in glob(train_path+'/images' + '/*.jpg' )]\n",
        "train_image_path_list += [x for x in glob(train_path+'/images' + '/*.png' )]\n",
        "train_image_path_list=sorted(train_image_path_list)\n",
        "\n",
        "train_label_path_list=sorted([x for x in glob(train_path +'/labels' +'/*.txt')])\n",
        "\n",
        "val_image_path_list= [x for x in glob(val_path+'/images' + '/*.jpg' )]\n",
        "val_image_path_list += [x for x in glob(val_path+'/images' + '/*.png' )]\n",
        "val_image_path_list = sorted(val_image_path_list)\n",
        "\n",
        "val_label_path_list= sorted([x for x in glob(val_path +'/labels' +'/*.txt')])\n",
        "\n",
        "train_image_path_list_selected = []\n",
        "labels_list_train_selected= []\n",
        "val_image_path_list_selected= []\n",
        "labels_list_val_selected = []\n",
        "\n",
        "for image in train_image_path_list:\n",
        "    for label in train_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            train_image_path_list_selected.append(image)\n",
        "            labels_list_train_selected.append(label)\n",
        "            break\n",
        "for image in val_image_path_list:\n",
        "    for label in val_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            val_image_path_list_selected.append(image)\n",
        "            labels_list_val_selected.append(label)\n",
        "            break\n",
        "train1 = dataset(train_image_path_list_selected)\n",
        "val1 = dataset(val_image_path_list_selected)\n",
        "\n",
        "labels_train_selected1=xywh(labels_list_train_selected)\n",
        "labels_val_selected1 = xywh(labels_list_val_selected)\n",
        "\n",
        "labels_train1 = labeling_1(labels_train_selected1)\n",
        "labels_val1 = labeling_1(labels_val_selected1)\n",
        "\n",
        "train = np.concatenate([train ,train1],axis=0)\n",
        "labels_train=np.concatenate([labels_train,labels_train1],axis=0)\n",
        "val = np.concatenate([val, val1],axis=0)\n",
        "labels_val= np.concatenate([labels_val, labels_val1],axis=0)\n",
        "\n",
        "len(train), len(labels_train),len(val) ,len(labels_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaambM5mW0Zg"
      },
      "source": [
        "추가데이터 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN8wIztVWy22",
        "outputId": "b3705692-186d-4a9b-e557-2980134da7d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4348, 4348, 270, 270)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def xml_parser(xml_path_list):\n",
        "    lst=[]\n",
        "    total=[]\n",
        "    for x in xml_path_list:\n",
        "        tree = ET.parse(x)\n",
        "        root = tree.getroot()\n",
        "        width=float(root[4][0].text)\n",
        "        height=float(root[4][1].text)\n",
        "        lst=[]\n",
        "        for i in range(len(root[6:])):\n",
        "            label_name=root[6+i][0].text\n",
        "            dic = {'fire':'0'}\n",
        "            label=dic[label_name]\n",
        "\n",
        "            xmin=float(root[6+i][4][0].text)\n",
        "            ymin=float(root[6+i][4][1].text)\n",
        "            xmax=float(root[6+i][4][2].text)\n",
        "            ymax=float(root[6+i][4][3].text)\n",
        "\n",
        "            x_center = (xmin+xmax)/2/width\n",
        "            y_center = (ymin+ymax)/2/height\n",
        "            w=(xmax-xmin)/width\n",
        "            h=(ymax-ymin)/height\n",
        "            string =label+ str(' ') +str(x_center)+str(' ')+str(y_center)+str(' ')+str(w)+str(' ') +str(h)\n",
        "            lst.append(string)\n",
        "        total.append(lst)\n",
        "\n",
        "    return total\n",
        "\n",
        "train_path = '/content/drive/MyDrive/yolo/fire-dataset/train'\n",
        "val_path = '/content/drive/MyDrive/yolo/fire-dataset/validation'\n",
        "\n",
        "train_image_path_list= [x for x in glob(train_path+'/images' + '/*.jpg' )]\n",
        "train_image_path_list += [x for x in glob(train_path+'/images' + '/*.png' )]\n",
        "train_image_path_list=sorted(train_image_path_list)\n",
        "\n",
        "train_label_path_list=sorted([x for x in glob(train_path +'/annotations' +'/*.xml')])\n",
        "\n",
        "val_image_path_list= [x for x in glob(val_path+'/images' + '/*.jpg' )]\n",
        "val_image_path_list += [x for x in glob(val_path+'/images' + '/*.png' )]\n",
        "val_image_path_list = sorted(val_image_path_list)\n",
        "\n",
        "val_label_path_list= sorted([x for x in glob(val_path +'/annotations' +'/*.xml')])\n",
        "\n",
        "train_image_path_list_selected = []\n",
        "labels_list_train_selected= []\n",
        "val_image_path_list_selected= []\n",
        "labels_list_val_selected = []\n",
        "\n",
        "for image in train_image_path_list:\n",
        "    for label in train_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            train_image_path_list_selected.append(image)\n",
        "            labels_list_train_selected.append(label)\n",
        "            break\n",
        "for image in val_image_path_list:\n",
        "    for label in val_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            val_image_path_list_selected.append(image)\n",
        "            labels_list_val_selected.append(label)\n",
        "            break\n",
        "\n",
        "train2 = dataset(train_image_path_list_selected)\n",
        "val2 = dataset(val_image_path_list_selected)\n",
        "\n",
        "labels_train_selected2=xml_parser(labels_list_train_selected)\n",
        "labels_val_selected2 = xml_parser(labels_list_val_selected)\n",
        "\n",
        "labels_train2 = labeling(labels_train_selected2)\n",
        "labels_val2 = labeling(labels_val_selected2)\n",
        "\n",
        "train = np.concatenate([train ,train2],axis=0)\n",
        "labels_train=np.concatenate([labels_train,labels_train2],axis=0)\n",
        "val = np.concatenate([val, val2],axis=0)\n",
        "labels_val= np.concatenate([labels_val, labels_val2],axis=0)\n",
        "\n",
        "len(train), len(labels_train),len(val) ,len(labels_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wESLCuXZex2"
      },
      "source": [
        "추가데이터 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Vi6mna6fW40G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d71cdb3-e71f-4825-d426-43cfa1eeb796"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4791, 4791, 316, 316)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_path = '/content/drive/MyDrive/yolo/fire/train'\n",
        "val_path = '/content/drive/MyDrive/yolo/fire/valid'\n",
        "\n",
        "train_image_path_list= [x for x in glob(train_path+'/images' + '/*.jpg' )]\n",
        "train_image_path_list += [x for x in glob(train_path+'/images' + '/*.png' )]\n",
        "train_image_path_list=sorted(train_image_path_list)\n",
        "\n",
        "train_label_path_list=sorted([x for x in glob(train_path +'/labels' +'/*.txt')])\n",
        "\n",
        "val_image_path_list= [x for x in glob(val_path+'/images' + '/*.jpg' )]\n",
        "val_image_path_list += [x for x in glob(val_path+'/images' + '/*.png' )]\n",
        "val_image_path_list = sorted(val_image_path_list)\n",
        "\n",
        "val_label_path_list= sorted([x for x in glob(val_path +'/labels' +'/*.txt')])\n",
        "\n",
        "train_image_path_list_selected = []\n",
        "labels_list_train_selected= []\n",
        "val_image_path_list_selected= []\n",
        "labels_list_val_selected = []\n",
        "\n",
        "for image in train_image_path_list:\n",
        "    for label in train_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            train_image_path_list_selected.append(image)\n",
        "            labels_list_train_selected.append(label)\n",
        "            break\n",
        "for image in val_image_path_list:\n",
        "    for label in val_label_path_list:\n",
        "        if Path(image).stem==Path(label).stem:\n",
        "            val_image_path_list_selected.append(image)\n",
        "            labels_list_val_selected.append(label)\n",
        "            break\n",
        "train3 = dataset(train_image_path_list_selected)\n",
        "val3 = dataset(val_image_path_list_selected)\n",
        "\n",
        "labels_train_selected3=xywh(labels_list_train_selected)\n",
        "labels_val_selected3 = xywh(labels_list_val_selected)\n",
        "\n",
        "labels_train3 = labeling(labels_train_selected3)\n",
        "labels_val3 = labeling(labels_val_selected3)\n",
        "\n",
        "train = np.concatenate([train ,train3],axis=0)\n",
        "labels_train=np.concatenate([labels_train,labels_train3],axis=0)\n",
        "val = np.concatenate([val, val3],axis=0)\n",
        "labels_val= np.concatenate([labels_val, labels_val3],axis=0)\n",
        "\n",
        "len(train), len(labels_train),len(val) ,len(labels_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmVvN1wSW2Wb"
      },
      "source": [
        "본문 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3Hyxhr-egLzD"
      },
      "outputs": [],
      "source": [
        "def IOU(label_box,pred_box,size=448):\n",
        "    # input1 = n 7 7 1 4\n",
        "    # input2 = n 7 7 1 4\n",
        "    \n",
        "    label_xy = label_box[...,:2]* size # n 7 7 1 2\n",
        "    label_wh = label_box[...,2:4]* size # n 7 7 1 2\n",
        "    pred_xy = pred_box[...,:2]* size # n 7 7 1 2\n",
        "    pred_wh =  pred_box[...,2:4]* size # n 7 7 1 2\n",
        "\n",
        "    label_min_xy = tf.math.subtract(label_xy , label_wh/2) # n 7 7 1 2\n",
        "    label_max_xy = tf.math.add(label_xy , label_wh/2)\n",
        "\n",
        "    pred_min_xy = tf.math.subtract(pred_xy , pred_wh/2)\n",
        "    pred_max_xy = tf.math.add(pred_xy , pred_wh/2)\n",
        "\n",
        "    intersect_mins = tf.math.maximum (label_min_xy,pred_min_xy) # n 7 7 1 2\n",
        "    intersect_maxs = tf.math.minimum (label_max_xy,pred_max_xy)\n",
        "\n",
        "    intersect_wh =  tf.math.maximum (intersect_maxs-intersect_mins,0)# 음수는 intersect =0\n",
        "    intersect_areas = intersect_wh[...,0]*intersect_wh[...,1] # n 7 7 1\n",
        "\n",
        "    true_areas = label_wh[...,0]*label_wh[...,1]# n 7 7 1 \n",
        "    pred_areas = pred_wh[...,0]*pred_wh[...,1]# n 7 7 1\n",
        "\n",
        "    union_areas= true_areas + pred_areas - intersect_areas# n 7 7 1\n",
        "    iou = intersect_areas / union_areas# n 7 7 1\n",
        " \n",
        "    return iou # n 7 7 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mUFZwGYAfeGk"
      },
      "outputs": [],
      "source": [
        "def YOLO_loss(y_true,y_pred):\n",
        "\n",
        "    tf.debugging.check_numerics(\n",
        "    y_pred, message='y_pred', name=None\n",
        ")    \n",
        "\n",
        "    label_cls = y_true[...,10:] # n 7 7 20\n",
        "    label_box = y_true[...,1:5] # n 7 7 4\n",
        "    confidence = y_true[...,0] # n 7 7\n",
        "    confidence_mask = tf.expand_dims(confidence,axis=3)# n 7 7 1\n",
        "\n",
        "    predict_cls = y_pred[...,10:] # n 7 7 20\n",
        "    predict_trust1= tf.expand_dims(y_pred[...,0], axis = 3)# n 7 7 1\n",
        "    predict_trust2= tf.expand_dims(y_pred[...,5],axis = 3)# n 7 7 1\n",
        "    predict_trust = tf.concat([predict_trust1,predict_trust2], axis= 3)# n 7 7 2\n",
        "\n",
        "    bbox1= y_pred[...,1:5]# n 7 7 4\n",
        "    bbox2= y_pred[...,5:9]# n 7 7 4\n",
        "\n",
        "\n",
        "    _label_box = tf.reshape(label_box,[-1,7,7,1,4])\n",
        "    _bbox1_pred = tf.reshape(bbox1,[-1,7,7,1,4])\n",
        "    _bbox2_pred = tf.reshape(bbox2,[-1,7,7,1,4])\n",
        "\n",
        "    iou_bbox1=IOU(_label_box,_bbox1_pred)+(1e-9)# n 7 7 1\n",
        "    iou_bbox2=IOU(_label_box,_bbox2_pred)# n 7 7 1\n",
        "\n",
        "\n",
        "    iou_bbox = tf.concat([iou_bbox1,iou_bbox2],axis=3 ) # n 7 7 2\n",
        "    best_box=tf.math.reduce_max(iou_bbox,axis=3,keepdims=True) # n 7 7 1\n",
        "\n",
        "    \n",
        "    box_mask = tf.cast(iou_bbox==best_box,dtype= tf.float32) # n 7 7 2\n",
        "\n",
        "\n",
        "    #confidence loss\n",
        "    no_object_loss= 0.5 * (1-tf.concat([confidence_mask,confidence_mask],axis=3) ) * tf.math.square(0 - predict_trust) # box_mask * confidence_mask = exist 1 no 0 / n 7 7 2\n",
        "    object_loss = box_mask*confidence_mask*tf.math.square(1-predict_trust)# n 7 7 2\n",
        "    confidence_loss= tf.math.reduce_sum(no_object_loss+object_loss) # shape= () , tensor\n",
        "    # class loss\n",
        "    cls_loss= confidence_mask * tf.math.square(label_cls - predict_cls)# n 7 7 20\n",
        "    cls_loss = tf.math.reduce_sum(cls_loss) # shape= () , tensor \n",
        "    #box loss\n",
        "\n",
        "    selected_box1 =  tf.expand_dims(box_mask[...,0],axis=3) * bbox1 # n 7 7 4  \n",
        "    selected_box2 =  tf.expand_dims(box_mask[...,1],axis=3) * bbox2 # n 7 7 4\n",
        "    selected_box = selected_box1+selected_box2# n 7 7 4\n",
        "\n",
        "    xy_label = label_box[...,0:2]\n",
        "    xy_pred = selected_box[...,0:2]\n",
        "\n",
        "    box_loss_xy = 5  * confidence_mask * tf.square(xy_label-xy_pred)# n 7 7 2\n",
        "    \n",
        "    wh_label = label_box[...,2:4]\n",
        "    wh_pred = selected_box[...,2:4]\n",
        "\n",
        "    zeros= tf.zeros(tf.shape(wh_label))\n",
        "\n",
        "    box_loss_wh = 5  * confidence_mask * tf.square(tf.math.sqrt(tf.math.maximum(wh_label,zeros))-tf.math.sqrt(tf.math.maximum(wh_pred,zeros)))# n 7 7 2\n",
        "\n",
        "    box_loss= tf.math.reduce_sum(box_loss_xy + box_loss_wh)\n",
        "    \n",
        "    # xy_pred2 = selected_box[...,4:6]\n",
        "    # box_loss_xy2 = 5  * confidence_mask * tf.square(xy_label-xy_pred2)\n",
        "    # wh_pred2 = selected_box[...,6:8]\n",
        "    # box_loss_wh2=5  * confidence_mask * tf.square(tf.math.sqrt(tf.math.maximum(wh_label,zeros))-tf.math.sqrt(tf.math.maximum(wh_pred2,zeros))) \n",
        "    # box_loss2=tf.math.reduce_sum(box_loss_xy2 + box_loss_wh2)\n",
        "\n",
        "    \n",
        "\n",
        "    loss= confidence_loss + cls_loss + box_loss\n",
        "    loss=loss/len(y_true)\n",
        "\n",
        "    return loss\n",
        "\n",
        "#     tf.debugging.check_numerics(\n",
        "#     box_loss_wh, message='box_loss_wh', name=None\n",
        "# )    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6w_0qGrQzlSe"
      },
      "outputs": [],
      "source": [
        "class YOLO_reshape(tf.keras.layers.Layer):\n",
        "    def __init__(self,target_shape=(7,7,30)):\n",
        "        super(YOLO_reshape,self).__init__()\n",
        "        self.target_shape = target_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update(\n",
        "            {\n",
        "                'target_shape': self.target_shape\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "    def call(self,input):# input= 1470\n",
        "        S= self.target_shape[0]\n",
        "        B = 2\n",
        "        C= 20\n",
        "\n",
        "        idx1= S*S*C\n",
        "        idx2= idx1 + S*S*B\n",
        "\n",
        "        input = tf.reshape(input,(-1,S*S*(B*5+C))) \n",
        "        \n",
        "        # class probabilities\n",
        "        class_probs= tf.reshape(\n",
        "            input[:,:idx1],\n",
        "            (tf.shape(input)[0],)+(S,S,C))# n 7 7 20\n",
        "        # class_probs= tf.nn.softmax(class_probs)\n",
        "        # confidence\n",
        "        confidence = tf.reshape(\n",
        "            input[:,idx1:idx2],\n",
        "            (tf.shape(input)[0],)+(S,S,B))\n",
        "        # confidence=tf.nn.sigmoid(confidence)\n",
        "        c1,c2 = tf.split(confidence, num_or_size_splits=2,axis=3)# n 7 7 1 x 2\n",
        "        # boxes\n",
        "        boxes = tf.reshape(\n",
        "            input[:,idx2:],\n",
        "            (tf.shape(input)[0],)+(S,S,B*4)\n",
        "        )\n",
        "        # boxes= tf.nn.sigmoid(boxes)\n",
        "        b1,b2= tf.split(boxes,num_or_size_splits=2,axis=3)# n 7 7 4 x 2\n",
        "\n",
        "\n",
        "        outputs= tf.concat([c1,b1,c2,b2,class_probs],axis=3)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0mAmdACrMLXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b06bf3-ed18-461e-cf47-25c825e74411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 7, 7, 1024)        4719616   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 4, 4, 1024)        9438208   \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 1024)        9438208   \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 1024)        9438208   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16384)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              67112960  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1470)              6022590   \n",
            "                                                                 \n",
            " yolo_reshape (YOLO_reshape)  (None, 7, 7, 30)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,884,478\n",
            "Trainable params: 106,169,790\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "leaky_lelu=tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "decay= l2(1e-4)\n",
        "input_shape= (224,224,3)\n",
        "initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "pre_trained_vgg = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "\n",
        "for layer in pre_trained_vgg.layers:\n",
        "    layer.trainable= False\n",
        "    # if (hasattr(layer,'activation'))==True:\n",
        "    #     layer.activation = leaky_lelu\n",
        "    #     layer.kernel_regularizer=initializer\n",
        "model= Sequential()\n",
        "model.add(pre_trained_vgg)\n",
        "model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay,kernel_initializer=initializer ))\n",
        "model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay , strides=(2,2),kernel_initializer=initializer))\n",
        "\n",
        "model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ,kernel_initializer=initializer))\n",
        "model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ,kernel_initializer=initializer))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation = leaky_lelu, kernel_regularizer=decay,kernel_initializer=initializer))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(S*S*(5*B+C), activation = leaky_lelu, kernel_regularizer=decay,kernel_initializer=initializer))\n",
        "model.add(YOLO_reshape(target_shape= (7,7,30)))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t7l7qbLp3CEU"
      },
      "outputs": [],
      "source": [
        "model.load_weights('/content/drive/MyDrive/yolo/yolo_v1_fire.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jZutGjioj1Qv"
      },
      "outputs": [],
      "source": [
        "# leaky_lelu=tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "# decay= l2(5e-4)\n",
        "# input_shape= (448,448,3)\n",
        "\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(filters = 64 , kernel_size= (7,7), strides= (2,2) , padding= 'same',input_shape= input_shape , activation = leaky_lelu , kernel_regularizer=decay ))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides = (2,2), padding='same'))\n",
        "\n",
        "# model.add(Conv2D(filters = 192 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides = (2,2), padding='same'))\n",
        "\n",
        "# model.add(Conv2D(filters = 128 , kernel_size= (1,1), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 256 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 256 , kernel_size= (1,1), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 512 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides = (2,2), padding='same'))\n",
        "\n",
        "# model.add(Conv2D(filters = 256 , kernel_size= (1,1), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 512 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 256 , kernel_size= (1,1), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 512 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 512 , kernel_size= (1,1), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides = (2,2), padding='same'))\n",
        "\n",
        "# model.add(Conv2D(filters = 512 , kernel_size= (1,1), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 512 , kernel_size= (1,1), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay , strides=(2,2)))\n",
        "\n",
        "# model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "# model.add(Conv2D(filters = 1024 , kernel_size= (3,3), padding= 'same' , activation = leaky_lelu, kernel_regularizer=decay ))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(4096))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(S*S*(5*B+C)))\n",
        "# model.add(YOLO_reshape(target_shape= (7,7,30)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HPk7sUkFFBNl"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCH = 1000\n",
        "\n",
        "\n",
        "def scheduler(epoch):\n",
        "    lr = 1e-3\n",
        "\n",
        "    if epoch <500:\n",
        "        return lr/(1+0.005*epoch)\n",
        "    else:\n",
        "        return lr/(1+0.005*(epoch-500))\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "early_stopping= tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=50\n",
        ")\n",
        "\n",
        "filename= '/content/drive/MyDrive/yolo/yolo_v1_fire_re.h5'\n",
        "checkpoint = ModelCheckpoint(filename,             # file명을 지정합니다\n",
        "                             verbose=1,            # 로그를 출력합니\n",
        "                             save_best_only=True,   # 가장 best 값만 저장합니다\n",
        "                             save_weights_only=True\n",
        "                            \n",
        "                            )\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(lr=0.0001,epsilon=1e-7)\n",
        "optimizer=tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.0001,\n",
        "    momentum=0.09,\n",
        "    nesterov=False,\n",
        "    name='SGD'\n",
        ")\n",
        "\n",
        "model.compile(loss= YOLO_loss,\n",
        "            optimizer=optimizer,run_eagerly=True,\n",
        "            \n",
        "             )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyjN0nWuprex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2d6281-6076-4fc3-f3c9-d13fcb313c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3056\n",
            "Epoch 1: val_loss improved from inf to 3.19354, saving model to /content/drive/MyDrive/yolo/yolo_v1_fire_re.h5\n",
            "75/75 [==============================] - 61s 744ms/step - loss: 0.3056 - val_loss: 3.1935 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3053\n",
            "Epoch 2: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 674ms/step - loss: 0.3053 - val_loss: 3.1944 - lr: 9.9502e-04\n",
            "Epoch 3/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3040\n",
            "Epoch 3: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 669ms/step - loss: 0.3040 - val_loss: 3.1945 - lr: 9.9010e-04\n",
            "Epoch 4/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3019\n",
            "Epoch 4: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3019 - val_loss: 3.1940 - lr: 9.8522e-04\n",
            "Epoch 5/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3045\n",
            "Epoch 5: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.3045 - val_loss: 3.1945 - lr: 9.8039e-04\n",
            "Epoch 6/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3019\n",
            "Epoch 6: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3019 - val_loss: 3.1969 - lr: 9.7561e-04\n",
            "Epoch 7/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3038\n",
            "Epoch 7: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3038 - val_loss: 3.1941 - lr: 9.7087e-04\n",
            "Epoch 8/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3032\n",
            "Epoch 8: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3032 - val_loss: 3.1959 - lr: 9.6618e-04\n",
            "Epoch 9/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3031\n",
            "Epoch 9: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3031 - val_loss: 3.1950 - lr: 9.6154e-04\n",
            "Epoch 10/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3032\n",
            "Epoch 10: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3032 - val_loss: 3.1957 - lr: 9.5694e-04\n",
            "Epoch 11/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3035\n",
            "Epoch 11: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3035 - val_loss: 3.1962 - lr: 9.5238e-04\n",
            "Epoch 12/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3028\n",
            "Epoch 12: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3028 - val_loss: 3.1964 - lr: 9.4787e-04\n",
            "Epoch 13/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3027\n",
            "Epoch 13: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3027 - val_loss: 3.1966 - lr: 9.4340e-04\n",
            "Epoch 14/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3016\n",
            "Epoch 14: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 669ms/step - loss: 0.3016 - val_loss: 3.1965 - lr: 9.3897e-04\n",
            "Epoch 15/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3022\n",
            "Epoch 15: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3022 - val_loss: 3.1957 - lr: 9.3458e-04\n",
            "Epoch 16/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3013\n",
            "Epoch 16: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3013 - val_loss: 3.1967 - lr: 9.3023e-04\n",
            "Epoch 17/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3026\n",
            "Epoch 17: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.3026 - val_loss: 3.1964 - lr: 9.2593e-04\n",
            "Epoch 18/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3008\n",
            "Epoch 18: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3008 - val_loss: 3.1972 - lr: 9.2166e-04\n",
            "Epoch 19/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3001\n",
            "Epoch 19: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3001 - val_loss: 3.1966 - lr: 9.1743e-04\n",
            "Epoch 20/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3005\n",
            "Epoch 20: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3005 - val_loss: 3.1964 - lr: 9.1324e-04\n",
            "Epoch 21/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3006\n",
            "Epoch 21: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3006 - val_loss: 3.1951 - lr: 9.0909e-04\n",
            "Epoch 22/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2995\n",
            "Epoch 22: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.2995 - val_loss: 3.1970 - lr: 9.0498e-04\n",
            "Epoch 23/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2994\n",
            "Epoch 23: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.2994 - val_loss: 3.1967 - lr: 9.0090e-04\n",
            "Epoch 24/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3001\n",
            "Epoch 24: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3001 - val_loss: 3.1964 - lr: 8.9686e-04\n",
            "Epoch 25/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3012\n",
            "Epoch 25: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3012 - val_loss: 3.1957 - lr: 8.9286e-04\n",
            "Epoch 26/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2989\n",
            "Epoch 26: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2989 - val_loss: 3.1958 - lr: 8.8889e-04\n",
            "Epoch 27/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3005\n",
            "Epoch 27: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3005 - val_loss: 3.1945 - lr: 8.8496e-04\n",
            "Epoch 28/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3001\n",
            "Epoch 28: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.3001 - val_loss: 3.1966 - lr: 8.8106e-04\n",
            "Epoch 29/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2991\n",
            "Epoch 29: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.2991 - val_loss: 3.1949 - lr: 8.7719e-04\n",
            "Epoch 30/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2996\n",
            "Epoch 30: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2996 - val_loss: 3.1951 - lr: 8.7336e-04\n",
            "Epoch 31/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2992\n",
            "Epoch 31: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 670ms/step - loss: 0.2992 - val_loss: 3.1955 - lr: 8.6957e-04\n",
            "Epoch 32/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3002\n",
            "Epoch 32: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.3002 - val_loss: 3.1970 - lr: 8.6580e-04\n",
            "Epoch 33/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.3001\n",
            "Epoch 33: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.3001 - val_loss: 3.1981 - lr: 8.6207e-04\n",
            "Epoch 34/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2993\n",
            "Epoch 34: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2993 - val_loss: 3.1966 - lr: 8.5837e-04\n",
            "Epoch 35/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2992\n",
            "Epoch 35: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.2992 - val_loss: 3.1980 - lr: 8.5470e-04\n",
            "Epoch 36/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2998\n",
            "Epoch 36: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.2998 - val_loss: 3.1981 - lr: 8.5106e-04\n",
            "Epoch 37/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2986\n",
            "Epoch 37: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2986 - val_loss: 3.1984 - lr: 8.4746e-04\n",
            "Epoch 38/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2994\n",
            "Epoch 38: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.2994 - val_loss: 3.1972 - lr: 8.4388e-04\n",
            "Epoch 39/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2980\n",
            "Epoch 39: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.2980 - val_loss: 3.1968 - lr: 8.4034e-04\n",
            "Epoch 40/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2979\n",
            "Epoch 40: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2979 - val_loss: 3.1970 - lr: 8.3682e-04\n",
            "Epoch 41/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2987\n",
            "Epoch 41: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.2987 - val_loss: 3.1976 - lr: 8.3333e-04\n",
            "Epoch 42/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2979\n",
            "Epoch 42: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2979 - val_loss: 3.1983 - lr: 8.2988e-04\n",
            "Epoch 43/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2974\n",
            "Epoch 43: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2974 - val_loss: 3.1977 - lr: 8.2645e-04\n",
            "Epoch 44/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2970\n",
            "Epoch 44: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 671ms/step - loss: 0.2970 - val_loss: 3.1969 - lr: 8.2305e-04\n",
            "Epoch 45/1000\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.2968\n",
            "Epoch 45: val_loss did not improve from 3.19354\n",
            "75/75 [==============================] - 50s 672ms/step - loss: 0.2968 - val_loss: 3.1967 - lr: 8.1967e-04\n",
            "Epoch 46/1000\n",
            "10/75 [===>..........................] - ETA: 42s - loss: 0.2784"
          ]
        }
      ],
      "source": [
        "hist = model.fit(train,\n",
        "         labels_train,\n",
        "         batch_size=BATCH_SIZE,\n",
        "         epochs=EPOCH,\n",
        "         callbacks=[callback,checkpoint], \n",
        "         validation_data=(val, labels_val))\n",
        "         \n",
        "# 5. 모델 학습과정 표시하기\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label = 'train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label = 'val loss')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiHb7jLG_lhY"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/yolo/yolo_v1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvk2ZVZ8N2IA"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(train[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSEr4gsVP_iO"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(val[1:2])\n",
        "a=labels_val[1:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYPSwae7ZHBp"
      },
      "outputs": [],
      "source": [
        "a[0][3][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyWPnNm5QSPh"
      },
      "outputs": [],
      "source": [
        "y_pred[0][3][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBL3e8mXQWpx"
      },
      "outputs": [],
      "source": [
        "pic=dataset(['/content/fire.jpg'])\n",
        "y_pred = model.predict(pic)\n",
        "box,cls=nms(model.predict(pic),iou_threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NtCcZVR_90-"
      },
      "outputs": [],
      "source": [
        "y_pred[0][0][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH56qKHDZWPI"
      },
      "outputs": [],
      "source": [
        "def iou_list (list1,list2):\n",
        "\n",
        "  x_c1,y_c1,w1,h1 = list1\n",
        "  x_c2,y_c2,w2,h2 = list2\n",
        "  x1_min= x_c1-(w1/2)\n",
        "  x1_max= x_c1+(w1/2)\n",
        "  y1_min = y_c1-(h1/2)\n",
        "  y1_max= y_c1+(h1/2)\n",
        "\n",
        "  x2_min= x_c2-(w2/2)\n",
        "  x2_max= x_c2+(w2/2)\n",
        "  y2_min = y_c2-(h2/2)\n",
        "  y2_max= y_c2+(h2/2)\n",
        "\n",
        "  intersect_min_x = max(x2_min,x1_min)\n",
        "  intersect_max_x = min(x2_max,x1_max)\n",
        "  intersect_min_y = max(y2_min,y1_min)\n",
        "  intersect_max_y = min(y2_max,y1_max)\n",
        "\n",
        "  intersect_w = max(intersect_max_x-intersect_min_x,0)\n",
        "  intersect_h = max(intersect_max_y-intersect_min_y,0)\n",
        "\n",
        "  intersect_area= intersect_w*intersect_h\n",
        "\n",
        "  area1 = w1*h1\n",
        "  area2= w2*h2\n",
        "\n",
        "  union_area= area1 + area2 - intersect_area\n",
        "\n",
        "  iou = intersect_area / union_area\n",
        "\n",
        "  return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7kU7my_ZX9Y"
      },
      "outputs": [],
      "source": [
        "def nms(y_pred,iou_threshold=0.5,ccp_threshold=0.2):\n",
        "    bboxes1=y_pred[0][...,0:5]\n",
        "    bboxes2=y_pred[0][...,5:10]\n",
        "\n",
        "    cls = y_pred[0][...,10:]\n",
        "    cls= np.reshape(cls,(49,20))\n",
        "\n",
        "    bboxes1=np.reshape(bboxes1,(49,5))\n",
        "    bboxes2=np.reshape(bboxes2,(49,5))\n",
        "\n",
        "    ccp1 = cls*bboxes1[...,0:1] # 49 , 20\n",
        "    ccp2 = cls*bboxes2[...,0:1]\n",
        "    ccp_mask1=tf.cast(ccp1>=ccp_threshold,dtype=tf.float32)\n",
        "    ccp_mask2=tf.cast(ccp2>=ccp_threshold,dtype=tf.float32)\n",
        "\n",
        "    ccp1 =ccp1*ccp_mask1\n",
        "    ccp2 = ccp2*ccp_mask2\n",
        "\n",
        "    bboxes1=np.concatenate([bboxes1,ccp1],axis=1)\n",
        "    bboxes2=np.concatenate([bboxes2,ccp2],axis=1)\n",
        "\n",
        "    bboxes = np.concatenate([bboxes1,bboxes2],axis=0)# 98 25\n",
        "\n",
        "    sorted_list= sorted(bboxes,reverse=True,key = lambda x: x[0])\n",
        "    sorted_list = np.array(sorted_list)\n",
        "\n",
        "    sorted_box= sorted_list[:,1:5]\n",
        "    selected_bbox_list= []\n",
        "    count=0\n",
        "\n",
        "    for i in range(len(sorted_box)):\n",
        "        for j in range(i+1,len(sorted_box)):\n",
        "            iou=iou_list(sorted_box[i],sorted_box[j])\n",
        "            if iou >= iou_threshold:\n",
        "                sorted_box[j]=np.zeros(4,)\n",
        "                count += 1\n",
        "        if count>=1:\n",
        "            selected_bbox_list.append(sorted_list[i])\n",
        "            count=0\n",
        "    box=[x[1:5] for x in selected_bbox_list]\n",
        "    clss = [np.argmax(x[10:]) for x in selected_bbox_list]\n",
        "\n",
        "    return box, clss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fnrkEb6wutx"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw4WyxkayURj"
      },
      "outputs": [],
      "source": [
        "a=[['1 0.211207 0.316794 0.118966 0.221374']]\n",
        "\n",
        "\n",
        "b=labeling(a)\n",
        "\n",
        "b[0][2][1][0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOwEJ-YNzpzO"
      },
      "outputs": [],
      "source": [
        "nms(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AmrthiGzPtp"
      },
      "outputs": [],
      "source": [
        "labels_val_selected"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "yolo_final.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}